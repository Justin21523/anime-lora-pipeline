# Yokai Watch Advanced Training Guide

å®Œæ•´çš„é€²éšè¨“ç·´æŒ‡å—ï¼Œæ¶µè“‹ç‰¹æ•ˆã€é¢¨æ ¼ã€å‹•ä½œå’Œ ControlNet è¨“ç·´ã€‚

**åˆ†é¡æ¨™æº–ç‰ˆæœ¬**: 2025-10 Yokai schema æ“´å……ç‰ˆ
**åƒç…§æ–‡ä»¶**: `docs/YOKAI_SCHEMA_EXTENDED.md`
**ä¾†æº**: å®˜æ–¹å¦–æ€ªæ‰‹éŒ¶éŠæˆ²/å‹•ç•«åœ°é»ã€Fandom Wiki åˆ†é¡

---

## ç›®éŒ„

1. [ç‰¹æ®Šå ´æ™¯è¨“ç·´](#ç‰¹æ®Šå ´æ™¯è¨“ç·´)
   - [å¬å–šç‰¹æ•ˆ LoRA](#å¬å–šç‰¹æ•ˆ-lora)
   - [æ”»æ“Šç‰¹æ•ˆ LoRA](#æ”»æ“Šç‰¹æ•ˆ-lora)
   - [å‹•ä½œåºåˆ— (AnimateDiff)](#å‹•ä½œåºåˆ—-animatediff)
2. [é¢¨æ ¼ LoRA è¨“ç·´](#é¢¨æ ¼-lora-è¨“ç·´)
   - [æ¦‚å¿µç†è§£](#æ¦‚å¿µç†è§£)
   - [åˆ†çµ„ç­–ç•¥](#åˆ†çµ„ç­–ç•¥)
   - [è§¸ç™¼è©è¨­è¨ˆ](#è§¸ç™¼è©è¨­è¨ˆ)
   - [è¨“ç·´åƒæ•¸](#è¨“ç·´åƒæ•¸)
3. [ControlNet è¨“ç·´](#controlnet-è¨“ç·´)
   - [Pose Control](#pose-control)
   - [Depth Control](#depth-control)
   - [çµ„åˆæ§åˆ¶](#çµ„åˆæ§åˆ¶)
4. [é€²éšæŠ€å·§](#é€²éšæŠ€å·§)
   - [LoRA çµ„åˆ](#lora-çµ„åˆ)
   - [é¢¨æ ¼æ··åˆ](#é¢¨æ ¼æ··åˆ)
   - [ç‰¹æ•ˆæ³¨å…¥](#ç‰¹æ•ˆæ³¨å…¥)
5. [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

---

## ç‰¹æ®Šå ´æ™¯è¨“ç·´

### å¬å–šç‰¹æ•ˆ LoRA

è¨“ç·´å¦–æ€ªå¬å–šæ™‚çš„è¯éº—ç‰¹æ•ˆï¼ˆé­”æ³•é™£ã€å…‰æŸã€ç²’å­ç­‰ï¼‰ã€‚

#### ç¬¬ä¸€æ­¥ï¼šæª¢æ¸¬å¬å–šå ´æ™¯

```bash
python3 scripts/tools/yokai_summon_scene_detector.py \
    /home/b0979/yokai_input_fast \
    --output-dir /path/to/summon_scenes \
    --extract-mode sample \
    --min-score 60.0
```

**åƒæ•¸èªªæ˜**ï¼š
- `--extract-mode`:
  - `key`: åªæå–é—œéµå¹€ï¼ˆæœ€å°‘åœ–ç‰‡ï¼Œæœ€é«˜å“è³ªï¼‰
  - `sample`: æ¡æ¨£æå–ï¼ˆå¹³è¡¡å“è³ªå’Œæ•¸é‡ï¼‰
  - `all`: å…¨éƒ¨æå–ï¼ˆæœ€å¤šåœ–ç‰‡ï¼‰
- `--min-score`: å ´æ™¯è©•åˆ†é–€æª»ï¼ˆ0-100ï¼‰
  - 60-70: ä¸€èˆ¬å¬å–šå ´æ™¯
  - 70-85: è¯éº—å¬å–šå ´æ™¯
  - 85+: è¶…è¯éº—å¬å–šå ´æ™¯

**é æœŸè¼¸å‡º**ï¼š
```
summon_scenes/
â”œâ”€â”€ S1.01/
â”‚   â”œâ”€â”€ scene_001234/       # å ´æ™¯ç›®éŒ„
â”‚   â”‚   â”œâ”€â”€ scene_001234_frame_001240.png
â”‚   â”‚   â”œâ”€â”€ scene_001234_frame_001242.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ scene_002456/
â””â”€â”€ summon_scenes_metadata.json  # å ´æ™¯è©•åˆ†å’Œå…ƒæ•¸æ“š
```

#### ç¬¬äºŒæ­¥ï¼šçµ„ç¹”ç‰¹æ•ˆé¡å‹

```bash
python3 scripts/tools/special_effects_organizer.py \
    summon_scenes \
    --output-dir organized_effects \
    --separate-layers
```

**è¼¸å‡ºçµæ§‹**ï¼š
```
organized_effects/
â”œâ”€â”€ by_type/
â”‚   â”œâ”€â”€ summon/              # å¬å–šé¡ç‰¹æ•ˆï¼ˆmedalå¬å–šã€mirapoã€portalï¼‰
â”‚   â”œâ”€â”€ attack_soultimate/   # å¿…æ®ºæŠ€ç‰¹æ•ˆï¼ˆå« cut-inã€å¤§å­—ï¼‰
â”‚   â”œâ”€â”€ attack_beam/         # å…‰æŸæ”»æ“Š
â”‚   â”œâ”€â”€ attack_aoe/          # ç¯„åœæ”»æ“Š
â”‚   â”œâ”€â”€ magic_circle/        # é­”æ³•é™£ã€å°å°é™£
â”‚   â”œâ”€â”€ transformation/      # è®Šèº«ç‰¹æ•ˆï¼ˆshadowsideã€godsideã€fusionï¼‰
â”‚   â”œâ”€â”€ device_watch/        # å¦–æ€ªéŒ¶å…‰ï¼ˆå¬å–šéŒ¶å…‰ã€éŒ¶ç›¤æ—‹è½‰ï¼‰
â”‚   â”œâ”€â”€ yokai_world_mist/    # å¦–ç•Œç´«éœ§
â”‚   â”œâ”€â”€ festival_night/      # ç¥­å…¸ç‰¹æ•ˆï¼ˆç‡ˆç± ã€ç…™ç«ï¼‰
â”‚   â”œâ”€â”€ pure_effect/         # **ç´”ç‰¹æ•ˆï¼ˆç„¡è§’è‰²ç•«é¢ï¼‰**
â”‚   â””â”€â”€ ambient/             # ç’°å¢ƒç‰¹æ•ˆ
â”œâ”€â”€ pure_effects/            # ç´”ç‰¹æ•ˆåœ–å±¤ï¼ˆç§»é™¤è§’è‰²ï¼‰
â””â”€â”€ combined/                # ç‰¹æ•ˆ+è§’è‰²çµ„åˆ
```

**é‡è¦ï¼šç´”ç‰¹æ•ˆæª¢æ¸¬**
- å·¥å…·æœƒè‡ªå‹•æª¢æ¸¬ã€Œåªæœ‰ç‰¹æ•ˆã€æ²’æœ‰è§’è‰²ã€çš„ç•«é¢
- é€™äº›ç•«é¢æœƒæ¨™è¨˜ç‚º `pure_effect` ä¸¦åŠ ä¸Š `is_pure_effect: true` çš„ metadata
- **é€™äº›ç•«é¢ä¸æ‡‰è©²ç”¨æ–¼è§’è‰² style/pose åˆ†æï¼Œé¿å…æ±™æŸ“è¨“ç·´è³‡æ–™**
- ç´”ç‰¹æ•ˆç•«é¢é©åˆç”¨æ–¼ã€Œç‰¹æ•ˆ LoRAã€è¨“ç·´ï¼Œå¯ä»¥å­¸ç¿’ç´”ç²¹çš„ç‰¹æ•ˆé¢¨æ ¼

#### ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ Captions

```bash
python3 scripts/tools/batch_generate_captions_yokai.py \
    organized_effects/by_type/summon \
    --caption-mode effects
```

**Caption ç¯„ä¾‹**ï¼š
```
"yokai summon effect, magic circle, bright light beams, particle effects, energy aura, glowing symbols"
```

#### ç¬¬å››æ­¥ï¼šæº–å‚™è¨“ç·´è³‡æ–™

```bash
python3 scripts/tools/prepare_yokai_lora_training.py \
    organized_effects/by_type/summon \
    --output-dir training_data/summon_effects \
    --lora-type effects
```

#### ç¬¬äº”æ­¥ï¼šè¨“ç·´ LoRA

```bash
accelerate launch train_network.py \
    --config_file training_data/summon_effects/configs/summon_effects_config.toml
```

**å»ºè­°è¨“ç·´åƒæ•¸**ï¼ˆç‰¹æ•ˆ LoRAï¼‰ï¼š
```toml
[model]
pretrained_model_name_or_path = "animefull-latest"

[network]
network_module = "networks.lora"
network_dim = 64              # ç‰¹æ•ˆç´°ç¯€è±å¯Œï¼Œä½¿ç”¨è¼ƒé«˜ç¶­åº¦
network_alpha = 32
network_train_unet_only = true

[training]
max_train_epochs = 25         # ç‰¹æ•ˆéœ€è¦æ›´å¤šè¨“ç·´
learning_rate = 1e-4
unet_lr = 1e-4
text_encoder_lr = 5e-5

lr_scheduler = "cosine_with_restarts"
lr_scheduler_num_cycles = 3   # å¤šæ¬¡å¾ªç’°æå‡ç´°ç¯€

# ç‰¹æ•ˆè¨“ç·´é—œéµè¨­å®š
enable_bucket = true
bucket_no_upscale = true      # ä¿æŒåŸå§‹ç‰¹æ•ˆè§£æåº¦
color_aug = false             # ä¸è¦æ”¹è®Šç‰¹æ•ˆé¡è‰²
flip_aug = false              # ä¸è¦ç¿»è½‰ç‰¹æ•ˆæ–¹å‘
```

#### ä½¿ç”¨æ–¹æ³•

ç”Ÿæˆå¬å–šç‰¹æ•ˆï¼š
```
Prompt: "jibanyan, yokai summon effect, magic circle, glowing"
LoRA: summon_effects.safetensors (weight: 0.7-0.9)
```

---

### æ”»æ“Šç‰¹æ•ˆ LoRA

é¡ä¼¼å¬å–šç‰¹æ•ˆï¼Œä½†é‡å°æˆ°é¬¥å ´æ™¯çš„æ”»æ“Šå‹•ç•«ã€‚

#### è¨“ç·´æµç¨‹

```bash
# 1. å¾çµ„ç¹”å¥½çš„ç‰¹æ•ˆä¸­é¸æ“‡æ”»æ“Šé¡å‹
mkdir attack_effects_training
cp -r organized_effects/by_type/attack/* attack_effects_training/

# 2. ç”Ÿæˆ Captions
python3 scripts/tools/batch_generate_captions_yokai.py \
    attack_effects_training \
    --caption-mode effects

# 3. æº–å‚™è¨“ç·´
python3 scripts/tools/prepare_yokai_lora_training.py \
    attack_effects_training \
    --output-dir training_data/attack_effects \
    --lora-type effects

# 4. è¨“ç·´
accelerate launch train_network.py \
    --config_file training_data/attack_effects/configs/attack_effects_config.toml
```

**Caption ç¯„ä¾‹**ï¼š
```
"yokai attack effect, energy blast, shockwave, impact particles, battle scene"
```

---

### å‹•ä½œåºåˆ— (AnimateDiff)

è¨“ç·´å¦–æ€ªçš„å‹•ä½œæ¨¡å¼ï¼ˆå‡ºå ´ã€æ”»æ“Šã€ç§»å‹•ç­‰ï¼‰ç”¨æ–¼ AnimateDiff motion LoRAã€‚

#### ç¬¬ä¸€æ­¥ï¼šæå–å‹•ä½œåºåˆ—

```bash
python3 scripts/tools/action_sequence_extractor.py \
    /home/b0979/yokai_input_fast \
    --output-dir motion_sequences \
    --lengths 16 32 \
    --format animatediff
```

**åƒæ•¸èªªæ˜**ï¼š
- `--lengths`: åºåˆ—é•·åº¦ï¼ˆ8/16/32/64 å¹€ï¼‰
  - 16: çŸ­å‹•ä½œï¼ˆå‡ºå ´ã€è½‰èº«ï¼‰
  - 32: ä¸­ç­‰å‹•ä½œï¼ˆæ”»æ“Šã€ç§»å‹•ï¼‰
  - 64: é•·å‹•ä½œï¼ˆå®Œæ•´å¬å–šåºåˆ—ï¼‰

**è¼¸å‡ºçµæ§‹**ï¼š
```
motion_sequences/
â”œâ”€â”€ S1.01_seq001234_len16_entrance/
â”‚   â”œâ”€â”€ 0000.png
â”‚   â”œâ”€â”€ 0001.png
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ 0015.png
â”‚   â””â”€â”€ metadata.json
â””â”€â”€ sequences_metadata.json
```

#### ç¬¬äºŒæ­¥ï¼šæŒ‰å‹•ä½œé¡å‹åˆ†é¡

```bash
# è‡ªå‹•åˆ†é¡å·²å®Œæˆï¼ŒæŸ¥çœ‹ metadata.json
cat motion_sequences/sequences_metadata.json | jq '.sequences[] | {name, action_type}'

# æ‰‹å‹•é¸æ“‡ç‰¹å®šé¡å‹
mkdir entrance_sequences
find motion_sequences -name "*_entrance" -type d -exec cp -r {} entrance_sequences/ \;
```

**å‹•ä½œé¡å‹**ï¼š
- `entrance`: å‡ºå ´å‹•ç•«
- `attack`: æ”»æ“Šå‹•ä½œ
- `special_effect`: ç‰¹æ•ˆå‹•ç•«
- `acceleration`: åŠ é€Ÿé‹å‹•
- `transformation`: è®Šèº«éç¨‹

#### ç¬¬ä¸‰æ­¥ï¼šAnimateDiff è¨“ç·´

**æ³¨æ„**ï¼šAnimateDiff motion LoRA è¨“ç·´éœ€è¦ç‰¹æ®Šçš„è¨“ç·´è…³æœ¬ã€‚

```bash
# ä½¿ç”¨ AnimateDiff è¨“ç·´è…³æœ¬ï¼ˆéœ€å¦å¤–å®‰è£ï¼‰
python train_motion_lora.py \
    --data_dir entrance_sequences \
    --output_dir motion_lora/entrance \
    --sequence_length 16 \
    --batch_size 1 \
    --learning_rate 1e-4 \
    --max_train_steps 5000
```

**è¨“ç·´è¦é»**ï¼š
- åºåˆ—é•·åº¦å¿…é ˆä¸€è‡´ï¼ˆå…¨éƒ¨ 16 æˆ–å…¨éƒ¨ 32ï¼‰
- æ‰¹æ¬¡å¤§å°é€šå¸¸ç‚º 1ï¼ˆé¡¯å­˜é™åˆ¶ï¼‰
- å­¸ç¿’ç‡æ¯”æ™®é€š LoRA ä½ï¼ˆ1e-4 ~ 5e-5ï¼‰
- è¨“ç·´æ­¥æ•¸è¼ƒå¤šï¼ˆ3000-10000 stepsï¼‰

#### ä½¿ç”¨æ–¹æ³•

```python
# åœ¨ AnimateDiff pipeline ä¸­ä½¿ç”¨
from diffusers import AnimateDiffPipeline, MotionAdapter

adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2")
pipe = AnimateDiffPipeline.from_pretrained(
    "animefull-latest",
    motion_adapter=adapter
)

# åŠ è¼‰ motion LoRA
pipe.load_lora_weights("motion_lora/entrance/entrance_motion.safetensors")

# ç”Ÿæˆå‹•ç•«
output = pipe(
    prompt="jibanyan, entrance animation",
    num_frames=16,
    guidance_scale=7.5
)
```

---

## é¢¨æ ¼ LoRA è¨“ç·´

### æ¦‚å¿µç†è§£

**é¢¨æ ¼ LoRA vs è§’è‰² LoRA**ï¼š

| é¡å‹ | ç›®æ¨™ | è§¸ç™¼è© | è¨“ç·´è³‡æ–™ |
|------|------|--------|----------|
| è§’è‰² LoRA | å­¸ç¿’ç‰¹å®šè§’è‰² | "jibanyan" | å–®ä¸€è§’è‰²çš„å¤šå¼µåœ–ç‰‡ |
| é¢¨æ ¼ LoRA | å­¸ç¿’å…±åŒé¢¨æ ¼ | "cat-type yokai" | å¤šå€‹ç›¸ä¼¼è§’è‰²çš„åœ–ç‰‡ |

**ç‚ºä»€éº¼éœ€è¦é¢¨æ ¼ LoRAï¼Ÿ**

1. **ç”Ÿæˆè®Šé«”**ï¼š
   - è§’è‰² LoRA: "ç”Ÿæˆå‰èƒ–å–µ"
   - é¢¨æ ¼ LoRA: "ç”Ÿæˆè²“å‹å¦–æ€ªé¢¨æ ¼çš„æ–°è§’è‰²"

2. **çµ„åˆä½¿ç”¨**ï¼š
   ```
   è§’è‰² LoRA (0.8) + é¢¨æ ¼ LoRA (0.5)
   = "å‰èƒ–å–µï¼Œä½†å…·æœ‰æ›´å¼·çš„è²“å‹å¦–æ€ªç‰¹å¾µ"
   ```

3. **æ•¸æ“šæ•ˆç‡**ï¼š
   - å–®ä¸€è§’è‰²è³‡æ–™ä¸è¶³ â†’ çµ„åˆå¤šå€‹ç›¸ä¼¼è§’è‰² â†’ å­¸ç¿’å…±åŒé¢¨æ ¼

---

### åˆ†çµ„ç­–ç•¥

#### ç¬¬ä¸€æ­¥ï¼šAI è‡ªå‹•åˆ†é¡

```bash
python3 scripts/tools/yokai_style_classifier.py \
    character_clusters \
    --output-json yokai_taxonomy.json \
    --threshold 0.3 \
    --sample-size 10
```

**äº’å‹•å¼å¯©æ ¸**ï¼š
```
[1/128] cluster_000
AI Classification:
  appearance:
    - animal_cat: 0.85
    - quadruped: 0.72
  attribute:
    - fire: 0.68
  style:
    - cute: 0.91

Action (a/m/s/q): a  # Accept
```

**åˆ†é¡ç¶­åº¦**ï¼š

1. **Appearance** (å¤–è§€)ï¼š
   - `animal_cat`: è²“å‹
   - `animal_dog`: çŠ¬å‹
   - `animal_bird`: é³¥å‹
   - `animal_dragon`: é¾å‹
   - `object_food`: é£Ÿç‰©å‹
   - `object_tool`: å·¥å…·å‹
   - `humanoid`: äººå‹
   - `ghost`: å¹½éˆå‹

2. **Attribute** (å±¬æ€§)ï¼š
   - `fire`: ç«
   - `water`: æ°´
   - `wind`: é¢¨
   - `thunder`: é›·
   - `earth`: åœŸ
   - `ice`: å†°
   - `light`: å…‰
   - `dark`: æš—

3. **Style** (é¢¨æ ¼)ï¼š
   - `cute`: å¯æ„›
   - `cool`: é…·ç‚«
   - `brave`: å‹‡æ•¢
   - `scary`: ææ€–
   - `funny`: æç¬‘

4. **Body Type** (é«”å‹)ï¼š
   - `quadruped`: å››è¶³
   - `bipedal`: äºŒè¶³
   - `flying`: é£›è¡Œ
   - `floating`: æ¼‚æµ®

#### ç¬¬äºŒæ­¥ï¼šå®šç¾©è¨“ç·´çµ„

å‰µå»º `concept_groups.json`ï¼š

```json
[
  {
    "name": "cat_type_yokai",
    "category": "appearance",
    "values": ["animal_cat"],
    "comment": "æ‰€æœ‰è²“å‹å¦–æ€ª"
  },
  {
    "name": "cute_yokai",
    "category": "style",
    "values": ["cute"],
    "comment": "å¯æ„›é¢¨æ ¼å¦–æ€ª"
  },
  {
    "name": "fire_water_dual",
    "category": "attribute",
    "values": ["fire", "water"],
    "comment": "ç«æ°´é›™å±¬æ€§"
  },
  {
    "name": "cute_cat_fire",
    "category": "multi",
    "filters": {
      "appearance": ["animal_cat"],
      "style": ["cute"],
      "attribute": ["fire"]
    },
    "comment": "å¯æ„›çš„ç«å±¬æ€§è²“å‹å¦–æ€ª"
  }
]
```

**åˆ†çµ„å»ºè­°**ï¼š

- **å–®ä¸€ç¶­åº¦** (æ¨è–¦åˆå­¸è€…)ï¼š
  ```json
  {"name": "cat_type", "category": "appearance", "values": ["animal_cat"]}
  ```

- **å¤šæ¨™ç±¤å–®ç¶­åº¦**ï¼š
  ```json
  {"name": "all_animals", "category": "appearance",
   "values": ["animal_cat", "animal_dog", "animal_bird"]}
  ```

- **å¤šç¶­åº¦çµ„åˆ** (é€²éš)ï¼š
  ```json
  {"name": "cute_animals", "category": "multi",
   "filters": {
     "appearance": ["animal_cat", "animal_dog"],
     "style": ["cute"]
   }}
  ```

#### ç¬¬ä¸‰æ­¥ï¼šæº–å‚™å¤šæ¦‚å¿µè¨“ç·´

```bash
python3 scripts/tools/multi_concept_lora_preparer.py \
    character_clusters \
    --taxonomy yokai_taxonomy.json \
    --output-dir multi_concept_training \
    --groups concept_groups.json \
    --training-type concept
```

**è¼¸å‡ºçµæ§‹**ï¼š
```
multi_concept_training/
â”œâ”€â”€ cat_type_yokai/
â”‚   â”œâ”€â”€ 12_cat_type_yokai/           # Repeat count = 12
â”‚   â”‚   â”œâ”€â”€ cluster_000_img001.png
â”‚   â”‚   â”œâ”€â”€ cluster_000_img001.txt   # "yokai, cat-type, char000, ..."
â”‚   â”‚   â”œâ”€â”€ cluster_005_img023.png
â”‚   â”‚   â”œâ”€â”€ cluster_005_img023.txt   # "yokai, cat-type, char005, ..."
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ cat_type_yokai_config.toml
â””â”€â”€ multi_concept_metadata.json
```

---

### è§¸ç™¼è©è¨­è¨ˆ

#### éšå±¤å¼è§¸ç™¼è©ç³»çµ±

```
Level 1: "yokai"                           â†’ é€šç”¨å¦–æ€ªé¢¨æ ¼
Level 2: "yokai, cat-type"                 â†’ è²“å‹å¦–æ€ªé¢¨æ ¼
Level 3: "yokai, cat-type, char000"        â†’ ç‰¹å®šè²“å‹å¦–æ€ª (cluster_000)
```

**ä½¿ç”¨ç¯„ä¾‹**ï¼š

1. **ç”Ÿæˆæ–°çš„è²“å‹å¦–æ€ª**ï¼š
   ```
   Prompt: "yokai, cat-type, orange fur, playful"
   â†’ ç”Ÿæˆå…·æœ‰è²“å‹é¢¨æ ¼çš„æ–°è§’è‰²
   ```

2. **ç”Ÿæˆç‰¹å®šè§’è‰²**ï¼š
   ```
   Prompt: "yokai, cat-type, char000, sitting"
   â†’ ç”Ÿæˆ cluster_000 çš„å¦–æ€ªï¼ˆåå§¿ï¼‰
   ```

3. **çµ„åˆå¤šå€‹å±¤ç´š**ï¼š
   ```
   Prompt: "yokai, cat-type, fire attribute, cute style"
   â†’ å¯æ„›çš„ç«å±¬æ€§è²“å‹å¦–æ€ª
   ```

#### è§¸ç™¼è©é…ç½®

åœ¨ `multi_concept_lora_preparer.py` ä¸­è‡ªå‹•ç”Ÿæˆï¼š

```python
# éšå±¤å¼
trigger_words = "yokai, cat-type, char000"

# æ‰å¹³å¼ï¼ˆå¯é¸ï¼‰
trigger_words = "cat_type_yokai_char000"
```

**é¸æ“‡å»ºè­°**ï¼š
- **éšå±¤å¼**: æ›´éˆæ´»ï¼Œå¯ä»¥åªç”¨éƒ¨åˆ†è§¸ç™¼è©
- **æ‰å¹³å¼**: æ›´ç°¡å–®ï¼Œä¸æœƒæ··æ·†

---

### è¨“ç·´åƒæ•¸

#### æ ¹æ“šçµ„å¤§å°è‡ªå‹•èª¿æ•´

```python
# 3 å€‹è§’è‰²ä»¥ä¸‹ - å­¸ç¿’å€‹é«” + å…±åŒé¢¨æ ¼
if num_characters <= 3:
    network_dim = 64      # é«˜ç¶­åº¦ä¿ç•™å€‹é«”ç‰¹å¾µ
    network_alpha = 32
    unet_lr = 1e-4
    max_train_epochs = 20

# 4-10 å€‹è§’è‰² - å¹³è¡¡å€‹é«”å’Œé¢¨æ ¼
elif num_characters <= 10:
    network_dim = 48
    network_alpha = 24
    unet_lr = 1.2e-4
    max_train_epochs = 18

# 10+ å€‹è§’è‰² - èšç„¦å…±åŒé¢¨æ ¼
else:
    network_dim = 32      # ä½ç¶­åº¦å¼·åˆ¶å­¸ç¿’å…±åŒç‰¹å¾µ
    network_alpha = 16
    unet_lr = 1.5e-4
    max_train_epochs = 15
```

#### æ‰‹å‹•èª¿æ•´æŠ€å·§

**å¦‚æœé¢¨æ ¼ä¸å¤ æ˜é¡¯**ï¼š
```toml
network_dim = 32          # é™ä½ç¶­åº¦
network_alpha = 16
unet_lr = 1.5e-4          # æé«˜å­¸ç¿’ç‡
max_train_epochs = 20     # å¢åŠ è¨“ç·´è¼ªæ•¸
```

**å¦‚æœå€‹é«”ç‰¹å¾µä¸Ÿå¤±**ï¼š
```toml
network_dim = 64          # æé«˜ç¶­åº¦
network_alpha = 32
unet_lr = 8e-5            # é™ä½å­¸ç¿’ç‡
max_train_epochs = 15
```

#### è¨“ç·´é…ç½®ç¯„ä¾‹

```toml
[model]
pretrained_model_name_or_path = "animefull-latest"

[network]
network_module = "networks.lora"
network_dim = 48                    # è‡ªå‹•è¨ˆç®—
network_alpha = 24
network_train_unet_only = true
network_dropout = 0.1               # é˜²æ­¢éæ“¬åˆ

[training]
max_train_epochs = 18
learning_rate = 1.2e-4
unet_lr = 1.2e-4
text_encoder_lr = 6e-5

lr_scheduler = "cosine_with_restarts"
lr_scheduler_num_cycles = 2

optimizer_type = "AdamW8bit"
optimizer_args = ["weight_decay=0.1"]  # é¢¨æ ¼è¨“ç·´ä½¿ç”¨è¼ƒé«˜ weight decay

# å¤šæ¦‚å¿µè¨“ç·´é—œéµè¨­å®š
shuffle_caption = true              # æ‰“äº‚ caption æå‡æ³›åŒ–
keep_tokens = 2                     # ä¿æŒ "yokai, cat-type" åœ¨å‰
caption_dropout_rate = 0.05         # è¼•å¾® dropout æå‡æ³›åŒ–

# æ•¸æ“šå¢å¼·
enable_bucket = true
color_aug = false                   # ä¿æŒé¢¨æ ¼é¡è‰²ä¸€è‡´æ€§
flip_aug = true                     # å¯ä»¥ç¿»è½‰
```

---

## ControlNet è¨“ç·´

### Pose Control

ä½¿ç”¨ OpenPose æ§åˆ¶å¦–æ€ªå§¿æ…‹ã€‚

#### ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆ OpenPose è³‡æ–™é›†

```bash
python3 scripts/tools/controlnet_complete_pipeline.py \
    character_clusters/cluster_000 \
    --output-dir controlnet_pose_data \
    --control-types openpose
```

**è¼¸å‡ºçµæ§‹**ï¼š
```
controlnet_pose_data/
â”œâ”€â”€ source/              # åŸåœ–
â”‚   â””â”€â”€ img001.png
â”œâ”€â”€ openpose/            # Pose éª¨æ¶åœ–
â”‚   â””â”€â”€ img001.png
â””â”€â”€ captions/
    â””â”€â”€ img001.txt
```

#### ç¬¬äºŒæ­¥ï¼šControlNet è¨“ç·´

ä½¿ç”¨ `sd-scripts` çš„ ControlNet è¨“ç·´è…³æœ¬ï¼š

```bash
accelerate launch train_controlnet.py \
    --pretrained_model_name_or_path="animefull-latest" \
    --train_data_dir="controlnet_pose_data" \
    --conditioning_image_column="openpose" \
    --image_column="source" \
    --caption_column="captions" \
    --resolution=512 \
    --learning_rate=1e-5 \
    --train_batch_size=4 \
    --max_train_steps=50000 \
    --output_dir="controlnet_pose_yokai"
```

**è¨“ç·´è¦é»**ï¼š
- ControlNet è¨“ç·´éœ€è¦æ›´å¤šè³‡æ–™ï¼ˆå»ºè­° 1000+ çµ„ï¼‰
- å­¸ç¿’ç‡è¼ƒä½ï¼ˆ1e-5ï¼‰
- è¨“ç·´æ™‚é–“é•·ï¼ˆ50k stepsï¼‰
- å»ºè­°ä½¿ç”¨å¤š GPU

#### ä½¿ç”¨æ–¹æ³•

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

controlnet = ControlNetModel.from_pretrained("controlnet_pose_yokai")
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "animefull-latest",
    controlnet=controlnet
)

# ä½¿ç”¨ OpenPose åœ–æ§åˆ¶ç”Ÿæˆ
image = pipe(
    prompt="jibanyan, cute pose",
    image=openpose_image,
    num_inference_steps=20
).images[0]
```

---

### Depth Control

ä½¿ç”¨æ·±åº¦åœ–æ§åˆ¶å ´æ™¯æ·±åº¦ã€‚

#### å‹•ç•«é¢¨æ ¼æ·±åº¦ç”Ÿæˆ

```bash
python3 scripts/tools/controlnet_complete_pipeline.py \
    character_clusters/cluster_000 \
    --output-dir controlnet_depth_data \
    --control-types depth \
    --background-dir layered_frames/background
```

**æ·±åº¦åœ–ç‰¹é»**ï¼š
- è§’è‰²å±¤ï¼šæ·±åº¦ 0.7-1.0ï¼ˆæœ€è¿‘ï¼‰
- èƒŒæ™¯å±¤ï¼šæ·±åº¦ 0.0-0.5ï¼ˆæœ€é ï¼‰
- é‚Šç·£ä¿æŒï¼ˆä¸æ¨¡ç³Šå‹•ç•«ç·šæ¢ï¼‰

#### è¨“ç·´æ–¹æ³•

åŒ Pose Controlï¼Œå°‡ `--conditioning_image_column` æ”¹ç‚º `depth`ã€‚

---

### çµ„åˆæ§åˆ¶

åŒæ™‚ä½¿ç”¨å¤šå€‹ ControlNetã€‚

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, MultiControlNetModel

# è¼‰å…¥å¤šå€‹ ControlNet
controlnet_pose = ControlNetModel.from_pretrained("controlnet_pose_yokai")
controlnet_depth = ControlNetModel.from_pretrained("controlnet_depth_yokai")

controlnets = MultiControlNetModel([controlnet_pose, controlnet_depth])

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "animefull-latest",
    controlnet=controlnets
)

# ä½¿ç”¨å¤šå€‹æ§åˆ¶åœ–
image = pipe(
    prompt="jibanyan, outdoor scene",
    image=[openpose_image, depth_image],
    controlnet_conditioning_scale=[0.8, 0.5],  # Pose æ¬Šé‡é«˜ï¼Œdepth æ¬Šé‡ä½
    num_inference_steps=20
).images[0]
```

---

## é€²éšæŠ€å·§

### LoRA çµ„åˆ

çµ„åˆå¤šå€‹ LoRA å¯¦ç¾è¤‡é›œæ•ˆæœã€‚

#### ç¯„ä¾‹ 1: è§’è‰² + ç‰¹æ•ˆ

```python
# è¼‰å…¥åŸºç¤æ¨¡å‹
pipe = StableDiffusionPipeline.from_pretrained("animefull-latest")

# è¼‰å…¥è§’è‰² LoRA (å‰èƒ–å–µ)
pipe.load_lora_weights("lora/jibanyan.safetensors", adapter_name="character")

# è¼‰å…¥ç‰¹æ•ˆ LoRA (å¬å–šç‰¹æ•ˆ)
pipe.load_lora_weights("lora/summon_effects.safetensors", adapter_name="effects")

# è¨­å®šæ¬Šé‡
pipe.set_adapters(["character", "effects"], adapter_weights=[0.8, 0.7])

# ç”Ÿæˆ
image = pipe(
    prompt="jibanyan, yokai summon effect, magic circle, glowing",
    num_inference_steps=30
).images[0]
```

**æ¬Šé‡å»ºè­°**ï¼š
- è§’è‰² LoRA: 0.7-0.9ï¼ˆä¸»é«”ï¼‰
- ç‰¹æ•ˆ LoRA: 0.5-0.8ï¼ˆè¼”åŠ©ï¼‰
- é¢¨æ ¼ LoRA: 0.4-0.6ï¼ˆå¾®èª¿ï¼‰

#### ç¯„ä¾‹ 2: è§’è‰² + é¢¨æ ¼ + ç‰¹æ•ˆ

```python
pipe.load_lora_weights("lora/jibanyan.safetensors", adapter_name="char")
pipe.load_lora_weights("lora/cat_type_yokai.safetensors", adapter_name="style")
pipe.load_lora_weights("lora/fire_effects.safetensors", adapter_name="fx")

pipe.set_adapters(["char", "style", "fx"], adapter_weights=[0.8, 0.5, 0.6])

image = pipe(
    prompt="jibanyan, cat-type yokai style, fire attribute, attack effect",
    num_inference_steps=30
).images[0]
```

**æ•ˆæœ**ï¼š
- `char (0.8)`: å‰èƒ–å–µçš„åŸºæœ¬å¤–è§€
- `style (0.5)`: å¼·åŒ–è²“å‹å¦–æ€ªç‰¹å¾µ
- `fx (0.6)`: æ·»åŠ ç«ç„°æ”»æ“Šç‰¹æ•ˆ

---

### é¢¨æ ¼æ··åˆ

æ··åˆä¸åŒé¢¨æ ¼ LoRA å‰µé€ æ–°é¢¨æ ¼ã€‚

#### è²“å‹ + çŠ¬å‹ = æ··åˆå‹

```python
pipe.load_lora_weights("lora/cat_type.safetensors", adapter_name="cat")
pipe.load_lora_weights("lora/dog_type.safetensors", adapter_name="dog")

# å„ 50% æ¬Šé‡
pipe.set_adapters(["cat", "dog"], adapter_weights=[0.5, 0.5])

image = pipe(
    prompt="yokai, animal hybrid, cute",
    num_inference_steps=30
).images[0]
```

#### å¯æ„› + é…·ç‚« = å¹³è¡¡é¢¨æ ¼

```python
pipe.load_lora_weights("lora/cute_yokai.safetensors", adapter_name="cute")
pipe.load_lora_weights("lora/cool_yokai.safetensors", adapter_name="cool")

pipe.set_adapters(["cute", "cool"], adapter_weights=[0.6, 0.4])

image = pipe(
    prompt="yokai, balanced style, adorable yet stylish"
).images[0]
```

---

### ç‰¹æ•ˆæ³¨å…¥

åœ¨è§’è‰²ç”Ÿæˆå¾ŒæœŸæ³¨å…¥ç‰¹æ•ˆã€‚

#### å…©éšæ®µç”Ÿæˆ

```python
# éšæ®µ 1: ç”Ÿæˆè§’è‰²ï¼ˆç„¡ç‰¹æ•ˆï¼‰
pipe.set_adapters(["character"], adapter_weights=[0.8])
latents = pipe(
    prompt="jibanyan, standing pose",
    num_inference_steps=30,
    output_type="latent"
).images

# éšæ®µ 2: æ³¨å…¥ç‰¹æ•ˆï¼ˆå¾ŒæœŸæ­¥é©Ÿï¼‰
pipe.set_adapters(["character", "effects"], adapter_weights=[0.6, 0.9])
image = pipe(
    prompt="jibanyan, summon effect, magic circle, glowing",
    latents=latents,
    num_inference_steps=15,  # é¡å¤– 15 æ­¥
    strength=0.5  # ä¿ç•™ 50% åŸåœ–
).images[0]
```

**å„ªé»**ï¼š
- è§’è‰²ç´°ç¯€ä¿ç•™æ›´å¥½
- ç‰¹æ•ˆæ›´è‡ªç„¶åœ°èåˆ

#### img2img + ç‰¹æ•ˆ

```python
# ä½¿ç”¨å·²ç”Ÿæˆçš„è§’è‰²åœ–
base_image = Image.open("jibanyan_base.png")

# åªè¼‰å…¥ç‰¹æ•ˆ LoRA
pipe.load_lora_weights("lora/summon_effects.safetensors")

# img2img æ·»åŠ ç‰¹æ•ˆ
image = pipe(
    prompt="yokai summon effect, magic circle, particle effects",
    image=base_image,
    strength=0.4,  # ä¿ç•™ 60% åŸåœ–
    num_inference_steps=20
).images[0]
```

---

## å¸¸è¦‹å•é¡Œ

### Q1: é¢¨æ ¼ LoRA è¨“ç·´å¾Œï¼Œè§’è‰²ç‰¹å¾µä¸Ÿå¤±äº†ï¼Ÿ

**åŸå› **ï¼šç¶­åº¦å¤ªä½æˆ–å­¸ç¿’ç‡å¤ªé«˜ã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```toml
network_dim = 64      # æé«˜ç¶­åº¦ (åŸæœ¬ 32)
network_alpha = 32
unet_lr = 8e-5        # é™ä½å­¸ç¿’ç‡ (åŸæœ¬ 1.5e-4)
max_train_epochs = 15 # æ¸›å°‘è¨“ç·´è¼ªæ•¸
```

### Q2: å¤šæ¦‚å¿µè¨“ç·´æ™‚ï¼Œè§¸ç™¼è©ä¸èµ·ä½œç”¨ï¼Ÿ

**æª¢æŸ¥**ï¼š
1. Caption ä¸­æ˜¯å¦åŒ…å«éšå±¤è§¸ç™¼è©ï¼Ÿ
   ```
   âœ“ "yokai, cat-type, char000, sitting"
   âœ— "char000, sitting"
   ```

2. `keep_tokens` è¨­å®šï¼š
   ```toml
   keep_tokens = 2  # ä¿æŒ "yokai, cat-type" åœ¨å‰
   ```

3. è¨“ç·´æ™‚æ˜¯å¦ä½¿ç”¨ `shuffle_caption`ï¼š
   ```toml
   shuffle_caption = true
   ```

### Q3: ç‰¹æ•ˆ LoRA ç”Ÿæˆçš„ç‰¹æ•ˆå¤ªå¼±ï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

1. **æé«˜ LoRA æ¬Šé‡**ï¼š
   ```python
   pipe.set_adapters(["effects"], adapter_weights=[0.9])  # å¾ 0.7 æé«˜åˆ° 0.9
   ```

2. **å¢å¼· Prompt**ï¼š
   ```
   "yokai summon effect, strong magic circle, very bright light beams,
    intense particle effects, glowing energy, spectacular"
   ```

3. **èª¿æ•´è¨“ç·´åƒæ•¸**ï¼š
   ```toml
   network_dim = 128     # æé«˜ç¶­åº¦æ•æ‰æ›´å¤šç‰¹æ•ˆç´°ç¯€
   max_train_epochs = 30 # æ›´å¤šè¨“ç·´
   ```

### Q4: ControlNet Pose ç”Ÿæˆæ™‚å§¿æ…‹ä¸æº–ç¢ºï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

1. **æª¢æŸ¥ OpenPose åœ–å“è³ª**ï¼š
   - éª¨æ¶æ˜¯å¦å®Œæ•´ï¼Ÿ
   - é—œç¯€ä½ç½®æ˜¯å¦æ­£ç¢ºï¼Ÿ

2. **æé«˜ ControlNet æ¬Šé‡**ï¼š
   ```python
   controlnet_conditioning_scale=1.0  # å¾ 0.8 æé«˜åˆ° 1.0
   ```

3. **ä½¿ç”¨æ›´å¥½çš„ OpenPose æ¨¡å‹**ï¼š
   ```python
   from controlnet_aux import OpenposeDetector
   openpose = OpenposeDetector.from_pretrained("lllyasviel/Annotators")
   ```

4. **è¨“ç·´å¦–æ€ªç‰¹å®šçš„ ControlNet**ï¼ˆéäººå‹ï¼‰ï¼š
   - æ”¶é›†æ›´å¤šéäººå‹å§¿æ…‹è³‡æ–™
   - ä½¿ç”¨å‹•ç‰©éª¨æ¶æ¨¡å‹

### Q5: çµ„åˆå¤šå€‹ LoRA æ™‚ï¼Œæ•ˆæœäº’ç›¸è¡çªï¼Ÿ

**åŸå› **ï¼šæ¬Šé‡ç¸½å’Œéé«˜æˆ– LoRA ä¹‹é–“ä¸å…¼å®¹ã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

1. **é™ä½ç¸½æ¬Šé‡**ï¼š
   ```python
   # âœ— ç¸½å’Œ = 2.4
   pipe.set_adapters(["char", "style", "fx"], adapter_weights=[0.9, 0.8, 0.7])

   # âœ“ ç¸½å’Œ = 1.7
   pipe.set_adapters(["char", "style", "fx"], adapter_weights=[0.7, 0.5, 0.5])
   ```

2. **å„ªå…ˆé †åº**ï¼š
   - ä¸»è¦ LoRA (è§’è‰²): 0.7-0.9
   - æ¬¡è¦ LoRA (é¢¨æ ¼/ç‰¹æ•ˆ): 0.4-0.6

3. **æ¸¬è©¦å…¼å®¹æ€§**ï¼š
   ```python
   # é€å€‹æ·»åŠ æ¸¬è©¦
   # 1. åªæœ‰è§’è‰²
   # 2. è§’è‰² + é¢¨æ ¼
   # 3. è§’è‰² + é¢¨æ ¼ + ç‰¹æ•ˆ
   ```

### Q6: AnimateDiff motion LoRA è¨“ç·´å¤±æ•—ï¼Ÿ

**å¸¸è¦‹å•é¡Œ**ï¼š

1. **åºåˆ—é•·åº¦ä¸ä¸€è‡´**ï¼š
   ```bash
   # ç¢ºä¿æ‰€æœ‰åºåˆ—é•·åº¦ç›¸åŒ
   find motion_sequences -name "*.png" | wc -l  # æ‡‰è©²æ˜¯ N * 16
   ```

2. **é¡¯å­˜ä¸è¶³**ï¼š
   ```bash
   # æ¸›å°‘æ‰¹æ¬¡å¤§å°
   --batch_size=1

   # ä½¿ç”¨æ¢¯åº¦æª¢æŸ¥é»
   --gradient_checkpointing
   ```

3. **å­¸ç¿’ç‡å¤ªé«˜**ï¼š
   ```bash
   # AnimateDiff å­¸ç¿’ç‡æ‡‰è©²æ›´ä½
   --learning_rate=5e-5  # è€Œé 1e-4
   ```

### Q7: ControlNet è¨“ç·´è³‡æ–™ä¸è¶³ï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

1. **æ•¸æ“šå¢å¼·**ï¼š
   ```python
   # ç¿»è½‰
   # æ—‹è½‰ï¼ˆå°è§’åº¦ï¼‰
   # ç¸®æ”¾
   ```

2. **çµ„åˆå¤šå€‹è§’è‰²çš„è³‡æ–™**ï¼š
   ```bash
   # åˆä½µæ‰€æœ‰ cluster çš„ ControlNet è³‡æ–™
   mkdir combined_controlnet_data
   cp -r controlnet_datasets/*/source/* combined_controlnet_data/source/
   cp -r controlnet_datasets/*/openpose/* combined_controlnet_data/openpose/
   ```

3. **ä½¿ç”¨é è¨“ç·´ ControlNet å¾®èª¿**ï¼š
   ```bash
   # å¾å®˜æ–¹ ControlNet é–‹å§‹
   --pretrained_controlnet="lllyasviel/control_v11p_sd15_openpose"
   # åªéœ€ 5k-10k steps å¾®èª¿
   ```

---

## ç¸½çµ

### è¨“ç·´æµç¨‹é€ŸæŸ¥

1. **ç‰¹æ•ˆ LoRA**:
   ```
   summon_detector â†’ effects_organizer â†’ caption â†’ prepare â†’ train
   ```

2. **é¢¨æ ¼ LoRA**:
   ```
   style_classifier â†’ groups.json â†’ multi_concept_preparer â†’ train
   ```

3. **Motion LoRA**:
   ```
   action_extractor â†’ AnimateDiff train
   ```

4. **ControlNet**:
   ```
   controlnet_pipeline â†’ ControlNet train
   ```

### æ¨è–¦è¨“ç·´é †åº

1. âœ… è§’è‰² LoRAï¼ˆåŸºç¤ï¼‰
2. âœ… é¢¨æ ¼ LoRAï¼ˆæ“´å±•ï¼‰
3. âœ… ç‰¹æ•ˆ LoRAï¼ˆå¢å¼·ï¼‰
4. ğŸ”§ ControlNetï¼ˆç²¾ç¢ºæ§åˆ¶ï¼‰
5. ğŸ”§ Motion LoRAï¼ˆå‹•ç•«ç”Ÿæˆï¼‰

### åƒè€ƒè³‡æº

- **åŸºç¤è¨“ç·´**: `USAGE_GUIDE.md`
- **å·¥å…·å¿«é€Ÿåƒè€ƒ**: `TOOLS_QUICK_REFERENCE.md`
- **é€²éšå·¥å…·**: `ADVANCED_FEATURES_QUICK_START.md`
- **kohya_ss æ–‡æª”**: https://github.com/kohya-ss/sd-scripts

---

**æœ€å¾Œæ›´æ–°**: 2025-10-30
**ç‰ˆæœ¬**: v1.0
**é©ç”¨æ–¼**: Yokai Watch LoRA è¨“ç·´é …ç›®
