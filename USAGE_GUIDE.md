# ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—èªªæ˜å¦‚ä½•ä½¿ç”¨è‡ªå‹•åŒ– pipeline è¨“ç·´é–ƒé›»åä¸€äººè§’è‰²çš„ LoRA æ¨¡å‹ã€‚

## å¿«é€Ÿé–‹å§‹

### å‰ç½®æ­¥é©Ÿï¼šå®‰è£ç’°å¢ƒ

```bash
# 1. å®‰è£ PyTorch (RTX 5080 å°ˆç”¨)
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128

# 2. å®‰è£å…¶ä»–ä¾è³´
pip install -r requirements.txt

# 3. é©—è­‰å®‰è£
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

## ä½¿ç”¨å ´æ™¯

### å ´æ™¯ Aï¼šä½¿ç”¨ç¾æœ‰çš„ Gold Standard åœ–ç‰‡ï¼ˆå††å ‚å®ˆï¼‰

ä½ å·²ç¶“æ‰‹å‹•æŒ‘é¸äº† 67 å¼µå††å ‚å®ˆçš„é«˜å“è³ªåœ–ç‰‡ã€‚ç¾åœ¨è¦æº–å‚™é€™äº›åœ–ç‰‡ç”¨æ–¼è¨“ç·´ï¼š

```bash
# ä¸€éµæº–å‚™è¨“ç·´æ•¸æ“š
python scripts/tools/prepare_training_data.py endou_mamoru --style detailed --repeat 10
```

é€™å€‹è…³æœ¬æœƒï¼š
1. âœ… ä½¿ç”¨ WD14 Tagger v3 ç‚ºæ¯å¼µåœ–ç‰‡ç”Ÿæˆè©³ç´°æ¨™è¨»
2. âœ… çµ„ç¹”æˆ kohya_ss è¨“ç·´æ ¼å¼ (`data/characters/endou_mamoru/training_ready/10_endou_mamoru/`)
3. âœ… ç”Ÿæˆé…å°çš„ `.txt` caption æ–‡ä»¶

**é æœŸè¼¸å‡ºï¼š**
- 67 å¼µåœ–ç‰‡ + 67 å€‹ caption æ–‡ä»¶
- æ¯å€‹ epoch æœƒé‡è¤‡ 10 æ¬¡ï¼ˆæœ‰æ•ˆè¨“ç·´æ¨£æœ¬ = 670ï¼‰

---

### å ´æ™¯ Bï¼šå¾å½±ç‰‡è‡ªå‹•æ”¶é›†è§’è‰²åœ–ç‰‡ï¼ˆå…¶ä»–è§’è‰²ï¼‰

ç•¶ä½ è¦è¨“ç·´å…¶ä»–è§’è‰²ä½†æ²’æœ‰è¶³å¤ åœ–ç‰‡æ™‚ï¼Œä½¿ç”¨å®Œæ•´ pipeline å¾å½±ç‰‡è‡ªå‹•æå–ï¼š

#### æ­¥é©Ÿ 1ï¼šæº–å‚™å½±ç‰‡å’Œé…ç½®

1. å°‡å‹•ç•«å½±ç‰‡æ”¾åˆ° `data/raw_videos/`
2. æº–å‚™å°‘é‡ï¼ˆ10-20å¼µï¼‰è©²è§’è‰²çš„ gold_standard åƒè€ƒåœ–
3. ç·¨è¼¯è§’è‰²é…ç½®æ–‡ä»¶ï¼ˆåƒè€ƒ `config/characters/endou_mamoru.yaml`ï¼‰

#### æ­¥é©Ÿ 2ï¼šåŸ·è¡Œå®Œæ•´ Pipeline

```bash
python scripts/pipeline/pipeline_orchestrator.py gouenji_shuuya \
  --videos data/raw_videos/S01E01.mkv data/raw_videos/S01E02.mkv
```

é€™æœƒè‡ªå‹•åŸ·è¡Œï¼š
1. ğŸ“¹ **VideoProcessor**: å¾å½±ç‰‡æå–å ´æ™¯é—œéµå¹€
2. ğŸ§¹ **ImageCleaner**: å»æ¨¡ç³Šã€å»é‡ã€å“è³ªéæ¿¾
3. ğŸ¯ **CharacterFilter**: å…©éšæ®µè§’è‰²è­˜åˆ¥ï¼ˆWD14 + CLIPï¼‰
4. ğŸ“ **AutoCaptioner**: ç”Ÿæˆè¨“ç·´æ¨™è¨»
5. ğŸ“¦ **PrepareTraining**: çµ„ç¹” kohya_ss æ ¼å¼

**é æœŸè¼¸å‡ºï¼š**
- è‡ªå‹•ç¯©é¸å‡ºè©²è§’è‰²çš„åœ–ç‰‡
- å·²ç”Ÿæˆæ¨™è¨»ä¸¦æº–å‚™è¨“ç·´

---

## é€²éšç”¨æ³•

### å–®ç¨ä½¿ç”¨å„æ¨¡çµ„

#### 1. åªç”Ÿæˆ Caption

```bash
python scripts/tools/caption_gold_standard.py endou_mamoru --style detailed
```

#### 2. åªåšè§’è‰²éæ¿¾

```bash
python scripts/pipeline/character_filter.py \
  data/candidates/ \
  --gold-standard data/characters/endou_mamoru/gold_standard/v1.0/images \
  --output data/filtered/ \
  --clip-threshold 0.75
```

#### 3. åªåšå½±ç‰‡è™•ç†

```bash
python scripts/pipeline/video_processor.py \
  data/raw_videos/S01E01.mkv \
  --output data/extracted_frames/
```

#### 4. åªåšå»é‡æ¸…ç†

```bash
python scripts/pipeline/image_cleaner.py \
  data/raw_images/ \
  --output data/cleaned/
```

---

## è¨“ç·´é…ç½®

### Kohya_ss è¨“ç·´åƒæ•¸

æº–å‚™å¥½è¨“ç·´æ•¸æ“šå¾Œï¼Œä½¿ç”¨ kohya_ss é€²è¡Œè¨“ç·´ï¼š

```bash
# å‡è¨­ä½ å·²å®‰è£ kohya_ss åˆ° models/sd-scripts/
cd models/sd-scripts/

python train_network.py \
  --pretrained_model_name_or_path="models/base_models/AnythingV5.safetensors" \
  --train_data_dir="../../data/characters/endou_mamoru/training_ready/" \
  --output_dir="../../models/loras/endou_mamoru/v1/" \
  --output_name="endou_mamoru_v1" \
  --network_module="networks.lora" \
  --network_dim=32 \
  --network_alpha=16 \
  --learning_rate=1e-4 \
  --lr_scheduler="cosine_with_restarts" \
  --train_batch_size=4 \
  --max_train_epochs=10 \
  --save_every_n_epochs=2 \
  --mixed_precision="fp16" \
  --optimizer_type="AdamW8bit" \
  --xformers \
  --cache_latents
```

**æ¨è–¦è¨“ç·´åƒæ•¸ï¼ˆ67 å¼µåœ–ç‰‡ï¼Œrepeat=10ï¼‰ï¼š**
- Epochs: 10-15
- Batch size: 4
- Network dim: 32
- Learning rate: 1e-4
- ç¸½è¨“ç·´æ­¥æ•¸ â‰ˆ (67 Ã— 10 Ã· 4) Ã— 10 = ~1675 steps

---
## é–‹å§‹è¨“ç·´

### æ–¹æ³• 1ï¼šä½¿ç”¨ä¾¿åˆ©è…³æœ¬ï¼ˆæ¨è–¦ï¼‰

```bash
cd /mnt/c/AI_LLM_projects/inazuma-eleven-lora
./train_endou_mamoru.sh
```

è…³æœ¬æœƒï¼š
1. æª¢æŸ¥ conda ç’°å¢ƒ
2. æª¢æŸ¥ GPU ç‹€æ…‹
3. é¡¯ç¤ºé…ç½®æ‘˜è¦
4. ç¢ºèªå¾Œå•Ÿå‹•è¨“ç·´

### æ–¹æ³• 2ï¼šç›´æ¥å‘½ä»¤

```bash
cd /mnt/c/AI_LLM_projects/sd-scripts

accelerate launch --num_cpu_threads_per_process=8 \
  train_network.py \
  --config_file=/mnt/c/AI_LLM_projects/inazuma-eleven-lora/train_endou_mamoru.toml
```

### è¨“ç·´æ—¥èªŒ
```
/mnt/c/AI_LLM_projects/ai_warehouse/outputs/inazuma-eleven/logs/endou_mamoru/
```

---

## ç›£æ§è¨“ç·´

### ä½¿ç”¨ TensorBoard

```bash
tensorboard --logdir=/mnt/c/AI_LLM_projects/ai_warehouse/outputs/inazuma-eleven/logs
```

åœ¨ç€è¦½å™¨ä¸­æ‰“é–‹ `http://localhost:6006` æŸ¥çœ‹ï¼š
- Loss curves
- Learning rate schedule
- Training metrics

---

## è¨“ç·´å¾Œæ¸¬è©¦

### 1. è¼‰å…¥ LoRA ä¸¦ç”Ÿæˆæ¸¬è©¦åœ–ç‰‡

å»ºç«‹æ¸¬è©¦è…³æœ¬ `test_lora.py`:

```python
from diffusers import StableDiffusionPipeline
import torch

# è¼‰å…¥ base model
pipe = StableDiffusionPipeline.from_single_file(
    "/mnt/c/AI_LLM_projects/ai_warehouse/models/stable-diffusion/anything-v4.5-vae-swapped.safetensors",
    torch_dtype=torch.float16
)

# è¼‰å…¥ LoRA
pipe.load_lora_weights(
    "/mnt/c/AI_LLM_projects/ai_warehouse/models/lora/character_loras/inazuma-eleven/endou_mamoru_v1.safetensors"
)

pipe.to("cuda")

# æ¸¬è©¦æç¤ºè©
test_prompts = [
    "endou_mamoru, smiling, looking at viewer, portrait",
    "endou_mamoru, serious expression, goalkeeper pose",
    "endou_mamoru, soccer field background, action pose",
    "endou_mamoru, school uniform, classroom",
]

for i, prompt in enumerate(test_prompts):
    image = pipe(
        prompt,
        num_inference_steps=28,
        guidance_scale=7.0,
        height=512,
        width=512
    ).images[0]

    image.save(f"test_endou_v1_{i+1}.png")
    print(f"âœ“ Generated: test_endou_v1_{i+1}.png")
```


## å°ˆæ¡ˆçµæ§‹

```
data/
â”œâ”€â”€ characters/
â”‚   â””â”€â”€ endou_mamoru/
â”‚       â”œâ”€â”€ gold_standard/v1.0/images/     # æ‰‹å‹•æŒ‘é¸çš„é«˜å“è³ªåœ–
â”‚       â”œâ”€â”€ auto_collected/                 # Pipeline è‡ªå‹•æ”¶é›†çš„åœ–
â”‚       â””â”€â”€ training_ready/10_endou_mamoru/ # è¨“ç·´å°±ç·’æ•¸æ“š
â”œâ”€â”€ raw_videos/                             # åŸå§‹å‹•ç•«å½±ç‰‡
â””â”€â”€ cache/                                  # CLIP å¿«å–

models/
â”œâ”€â”€ base_models/                            # SD åŸºç¤æ¨¡å‹
â”œâ”€â”€ loras/                                  # è¨“ç·´å®Œæˆçš„ LoRA
â””â”€â”€ sd-scripts/                             # kohya_ss è¨“ç·´è…³æœ¬

config/
â”œâ”€â”€ global_config.yaml                      # å…¨åŸŸé…ç½®
â””â”€â”€ characters/
    â”œâ”€â”€ endou_mamoru.yaml                   # è§’è‰²é…ç½®
    â”œâ”€â”€ gouenji_shuuya.yaml
    â””â”€â”€ ...

scripts/
â”œâ”€â”€ pipeline/                               # æ ¸å¿ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ video_processor.py
â”‚   â”œâ”€â”€ image_cleaner.py
â”‚   â”œâ”€â”€ character_filter.py
â”‚   â”œâ”€â”€ auto_captioner.py
â”‚   â””â”€â”€ pipeline_orchestrator.py
â”œâ”€â”€ tools/                                  # å·¥å…·è…³æœ¬
â”‚   â”œâ”€â”€ prepare_training_data.py
â”‚   â””â”€â”€ caption_gold_standard.py
â””â”€â”€ utils/                                  # å·¥å…·å‡½æ•¸
```

---

## å¸¸è¦‹å•é¡Œ

### Q1: CUDA out of memory

**æ–¹æ¡ˆï¼š**
- é™ä½ batch_sizeï¼ˆåœ¨ CharacterFilter ä¸­èª¿æ•´ï¼‰
- ä½¿ç”¨è¼ƒå°çš„ CLIP æ¨¡å‹ï¼ˆViT-B/32 è€Œé ViT-L/14ï¼‰
- åœ¨é…ç½®ä¸­å•Ÿç”¨ `enable_xformers: true`

### Q2: WD14 Tagger æ¨ç†ç·©æ…¢

**æ–¹æ¡ˆï¼š**
- ç¢ºèªå®‰è£çš„æ˜¯ `onnxruntime-gpu` è€Œé `onnxruntime`
- æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨

### Q3: Character Filter èª¤åˆ¤ç‡é«˜

**æ–¹æ¡ˆï¼š**
- å¢åŠ  gold_standard åƒè€ƒåœ–æ•¸é‡ï¼ˆæ¨è–¦ 20+ å¼µï¼‰
- èª¿æ•´ `clip_threshold`ï¼ˆé™ä½é–¾å€¼æœƒä¿ç•™æ›´å¤šåœ–ç‰‡ï¼‰
- æª¢æŸ¥ `required_tags` å’Œ `forbidden_tags` æ˜¯å¦éæ–¼åš´æ ¼

### Q4: Caption å“è³ªä¸ä½³

**æ–¹æ¡ˆï¼š**
- èª¿æ•´ `general_threshold`ï¼ˆæé«˜é–¾å€¼ = æ›´ç²¾ç¢ºä½†æ›´å°‘æ¨™ç±¤ï¼‰
- ä½¿ç”¨ `detailed` style è€Œé `minimal`
- æ‰‹å‹•ç·¨è¼¯ç”Ÿæˆçš„ `.txt` æ–‡ä»¶

---

## å¦‚æœéœ€è¦èª¿æ•´

### å•é¡Œ 1: Loss ä¸‹é™å¤ªæ…¢
```toml
# å¢åŠ  learning rate
learning_rate = 2e-4  # å¾ 1e-4 å¢åŠ 
```

### å•é¡Œ 2: éæ“¬åˆ (Overfitting)
```toml
# æ¸›å°‘ epochs
max_train_epochs = 6  # å¾ 10 æ¸›å°‘

# æˆ–å¢åŠ  regularization
network_alpha = 8  # å¾ 16 æ¸›å°‘
```

### å•é¡Œ 3: CUDA OOM (è¨˜æ†¶é«”ä¸è¶³)
```toml
# æ¸›å°‘ batch size
batch_size = 2  # å¾ 4 æ¸›å°‘

# å¢åŠ  gradient accumulation
gradient_accumulation_steps = 2
```
---

## BLIP-2 å‡ç´š (å¯é¸)

ç•¶ BLIP-2 æ¨¡å‹ä¸‹è¼‰å®Œæˆå¾Œï¼Œå¯ä»¥ç”Ÿæˆæ›´è©³ç´°çš„æ¨™è¨»ä¸¦é‡æ–°è¨“ç·´ v2:

```bash
# 1. æ¸¬è©¦ BLIP-2
conda run -n blip2-env python3 /tmp/test_blip2.py

# 2. ç”Ÿæˆ BLIP-2 æ¨™è¨»
conda run -n blip2-env python3 scripts/tools/generate_captions_blip2.py endou_mamoru

# 3. é‡æ–°æº–å‚™è¨“ç·´æ•¸æ“š
python3 scripts/tools/prepare_training_data.py endou_mamoru --style detailed --repeat 10

# 4. ä¿®æ”¹é…ç½®æ–‡ä»¶
# å°‡ output_name æ”¹ç‚º "endou_mamoru_v2"

# 5. é‡æ–°è¨“ç·´
./train_endou_mamoru.sh
```
## æ•…éšœæ’é™¤

### è¨“ç·´ç„¡æ³•å•Ÿå‹•

**æª¢æŸ¥ç’°å¢ƒ**:
```bash
conda activate env-ai
python3 -c "import library; print('âœ“ sd-scripts OK')"
nvidia-smi
```

**æª¢æŸ¥è·¯å¾‘**:
```bash
# ç¢ºèªæ‰€æœ‰è·¯å¾‘å­˜åœ¨
ls /mnt/c/AI_LLM_projects/ai_warehouse/training_data/inazuma-eleven/characters/endou_mamoru/training_ready/10_endou_mamoru/
ls /mnt/c/AI_LLM_projects/ai_warehouse/models/stable-diffusion/anything-v4.5-vae-swapped.safetensors
```

### è¨“ç·´ä¸­æ–·

```bash
# æª¢æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„ checkpoint
ls -lh /mnt/c/AI_LLM_projects/ai_warehouse/models/lora/character_loras/inazuma-eleven/

# å¦‚æœæœ‰ checkpointï¼Œå¯ä»¥ä¿®æ”¹é…ç½®ç¹¼çºŒè¨“ç·´ï¼š
# åœ¨ train_endou_mamoru.toml ä¸­æ·»åŠ :
# resume = "/path/to/endou_mamoru_v1-000004.safetensors"
```

å¦‚é‡å•é¡Œï¼Œè«‹æª¢æŸ¥ï¼š
1. `outputs/logs/` ä¸­çš„æ—¥èªŒæ–‡ä»¶
2. å„éšæ®µè¼¸å‡ºçš„ `*_report.json` æ–‡ä»¶
3. GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
