#!/usr/bin/env python3
"""
Advanced Pose Extractor

Advanced pose extraction with special handling for yokai:
- OpenPose/DWPose support for humanoid characters
- Special handling for non-humanoid yokai (quadruped, flying, multi-limb)
- Pose quality filtering
- Keypoint confidence scoring
- Sequence continuity checks

Generates high-quality pose data for ControlNet Pose training.
"""

import cv2
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
from typing import List, Dict, Optional, Tuple
import json
from datetime import datetime
from tqdm import tqdm
import warnings

warnings.filterwarnings("ignore")

try:
    from controlnet_aux import OpenposeDetector

    OPENPOSE_AVAILABLE = True
except ImportError:
    OPENPOSE_AVAILABLE = False
    print("âš ï¸  controlnet_aux not available, using fallback pose detection")


class AdvancedPoseExtractor:
    """
    Advanced pose extractor with yokai-specific handling
    """

    def __init__(self, device: str = "cuda"):
        self.device = device

        # Initialize OpenPose if available
        if OPENPOSE_AVAILABLE:
            print("ðŸ”§ Loading OpenPose detector...")
            try:
                self.openpose = OpenposeDetector.from_pretrained(
                    "lllyasviel/ControlNet"
                )
                print("  âœ“ OpenPose loaded")
            except Exception as e:
                print(f"  âš ï¸  OpenPose loading failed: {e}")
                self.openpose = None
        else:
            self.openpose = None

        # Yokai body types (FULL version derived from Yo-kai Watch Wiki categories)
        self.body_types = [
            # --- A. æ¨™æº–äººåž‹ / æŽ¥è¿‘äººåž‹ï¼ˆOpenPose ä¸»åŠ›ï¼‰ ---
            "humanoid_standard",  # æ­£å¸¸æ¯”ä¾‹äººåž‹ (student, samurai, idol)  :contentReference[oaicite:4]{index=4}
            "humanoid_chibi",  # Q ç‰ˆã€é ­å¤§è…³çŸ­çš„äººåž‹ â†’ å¾ˆå¤šå‰èƒ–å–µè¡ç”Ÿ
            "humanoid_longlimb",  # æ‰‹è…³éŽé•·ã€é—œç¯€æ‹‰å¾ˆé–‹çš„é™°å½±/Shadowsideå½¢æ…‹
            "oni_ogre",  # è§’ã€å·¨å¤§ä¸ŠåŠèº«ã€è…°å¸ƒ â†’ å°æ‡‰ Category:Oni Yo-kai :contentReference[oaicite:5]{index=5}
            "tengu_winged",  # äººåž‹ + ç¿…è†€ + é•·é¼» â†’ Category:Tengu :contentReference[oaicite:6]{index=6}
            "butler_ghost",  # Whisper é¡žï¼šäººåž‹ä¸ŠåŠèº« + é¬¼å°¾ + æ¼‚æµ®
            # --- B. å‹•ç‰©ï¼ç¸åž‹ from wiki çš„ Animal / Kappa / Komainu ---
            "bipedal_animal",  # ç«™èµ·ä¾†èµ°çš„è²“çŠ¬ç†Š â†’ Jibanyanã€Komasanç›´ç«‹ç‰ˆ
            "quadruped_beast",  # å››è¶³ç¸ï¼šç…å­ç‹—ã€ç‹›çŠ¬ã€è€è™Žã€ç…å­ã€ç‹¼  :contentReference[oaicite:7]{index=7}
            "kappa_aquatic",  # æ²³ç«¥é«”åž‹ï¼šé ­é ‚ç›¤ã€æ°´é¾œæ®¼ã€çŸ­å››è‚¢  :contentReference[oaicite:8]{index=8}
            "komainu_guardian",  # ç‹›çŠ¬é«”åž‹ï¼šèƒ¸å‰é¬ƒæ¯›ã€ç…å­ç‹—åƒã€å¤šåŠå››è¶³æˆ–åŠç›´ç«‹  :contentReference[oaicite:9]{index=9}
            "serpentine_longbody",  # è›‡/é¾çš„é•·æ¢èº«é«”ï¼Œä¸ä¸€å®šæœ‰è…³
            "dragon_beast",  # æ˜Žç¢ºæ˜¯é¾ã€å¸¶ç¿…æˆ–å¤šè§’ â†’ Category:Dragon Yo-kai :contentReference[oaicite:10]{index=10}
            "centaur_like",  # äººä¸ŠåŠèº« + ç¸ä¸‹åŠèº« / é¨Žä¹˜ä¸€é«”
            "avian_beast",  # é³¥åž‹ã€çŒ›ç¦½ã€æœƒé£›ä½†ä¸æ˜¯å¦–é›² â†’ è·Ÿé£›è¡Œåˆ†é–‹
            "aquatic_fishlike",  # é­šã€æ²³ç«¥é€²åŒ–ã€æ°´å¦–ã€çƒé¾œåž‹
            "insect_like",  # ç”²èŸ²ã€é¬å½¢èŸ²ã€è´è¶ã€æ˜†èŸ²æ­¦è€…ï¼ˆBeetler é‚£æŽ›ï¼‰
            "plant_rooted_beast",  # æ¤ç‰©/æ¨¹ç²¾é¡žï¼Œä½†å§¿å‹¢åƒç¸ â†’ æœ‰äº› event å¦–æ€ªæœƒé€™æ¨£
            # --- C. é£›è¡Œï¼æ¼‚æµ® å…©è·¯ ---
            "flying_selfpowered",  # å®˜æ–¹ç¨ç«‹çš„ã€Œèƒ½è‡ªå·±é£›ã€é‚£ç¾¤ â†’ æœ‰ç¿…/æ°£æµï¼Œä¸é å¤–ç‰© :contentReference[oaicite:11]{index=11}
            "cloud_rider",  # é¨Žåœ¨é›²ä¸Š / é¨Žåœ¨åé¨Žä¸Š â†’ Category:Yo-kai riding on a cloud :contentReference[oaicite:12]{index=12}
            "floating_spirit",  # æ²’è…³ã€å°¾å·´å¾€ä¸‹ã€é«”ç©å°ï¼šWhisperã€ç…™éœ§ã€ç‡ˆç± 
            "jellyfish_floating",  # æ°´æ¯/å‚˜åž‹å¾€ä¸‹åž‚ â†’ å¾ˆå¤šæ°´ç³»çœ‹èµ·ä¾†åƒé€™æ¨£
            # --- D. å¤šè‚¢ï¼åˆ†æ®µï¼ç¾¤é«” ---
            "multi_limb_tentacle",  # å¤šæ‰‹å¤šè…³ã€å¤šè§¸æ‰‹ã€ç« é­šåž‹
            "segmented_body",  # ä¸€ç¯€ä¸€ç¯€çš„èº«é«”ã€ç”²æ®¼é‡è¤‡ã€OpenPose æœƒæ–·çš„
            "swarm_group",  # Category:Group Yo-kai â†’ ä¸€å¼µåœ–ä¸€ç¾¤åŒåž‹å¦–æ€ª :contentReference[oaicite:13]{index=13}
            "fusion_compound",  # å…©éš»é»ä¸€èµ· / åˆé«”åž‹ / å·¦å³å°ç¨±æˆå…©å¦–
            "parasite_on_host",  # æ˜Žé¡¯é™„è‘—åœ¨äººã€å‹•ç‰©æˆ–ç‰©é«”ä¸Š â†’ ä½ è¦å…ˆåµæ¸¬ host
            # --- E. æ©Ÿæ¢° / è¼‰å…· / ç‰©ä»¶å¦–æ€ª ---
            "robot_mecha",  # Category:Robot Yo-kai â†’ Robonyanã€Robo- ç³»ã€é‹¼è£ç‰ˆ :contentReference[oaicite:14]{index=14}
            "armor_samurai",  # æ­¦è€…ã€å…¨èº«éŽ§ã€é‡‘å±¬æ¿å¾ˆå¤š â†’ Shadowside ä¹Ÿå¾ˆå¤š
            "vehicle_form",  # è»Šã€èˆ¹ã€ç«è»Šã€äº¤é€šå·¥å…·æˆå¦–
            "object_furniture",  # å®¶å…·æˆå¦–ï¼šæ«ƒå­ã€ç®±å­ã€æ¡Œå­ã€é˜ â†’ Inanimate Object Yo-kai è£¡ä¸€å¤§ç¥¨ :contentReference[oaicite:15]{index=15}
            "object_tool_weapon",  # æ­¦å™¨ã€å‚˜ã€åˆ·å­ã€ç­†ã€æ§Œå­æˆå¦–
            "object_accessory_mask",  # é¢å…·ã€é ­ç›”ã€è£é£¾ç‰©ã€é¢ç½©åž‹
            "object_paper_umbrella",  # å”å‚˜ / ç´™å¼µé¡ž â†’ çœŸçš„æœ‰ï¼ŒTVTropes ä¹Ÿæœ‰ umbrella spirit çš„æè¿° :contentReference[oaicite:16]{index=16}
            # --- F. åŠèº« / ç‰¹æ®Šå‡ºç¾æ–¹å¼ ---
            "partial_upper_body",  # åªæœ‰ä¸ŠåŠèº«å¾žç‰†/åœ°å†’å‡ºä¾† â†’ å¾ˆå¤šä»»å‹™åž‹å¦–æ€ªé€™æ¨£
            "head_only",  # åªæœ‰é ­åœ¨ç•«é¢ä¸Šï¼ˆå·¨å¤§è‡‰ã€é¢å…·å¦–æ€ªï¼‰
            "hand_arm_only",  # åªæœ‰æ‰‹oræ‰‹+ä¸€é»žè‚©è†€ â†’ é–€æŠŠæ‰‹ã€æ‹›æ‰‹å¦–æ€ª
            "wall_attached",  # é»ç‰†/é»é–€/è²¼åœ¨ç‰©é«”ä¸Šçš„ â†’ ControlNet è¦æ”¹ bbox
            # --- G. æ°´/é»æ¶²/è»Ÿé«” ---
            "aquatic_mermaid",  # åŠäººåŠé­š/æ°´å°¾å·´
            "slime_blob",  # è»Ÿå¡Šã€ç³¯ç±³ã€æ³¥æ²¼ â†’ å¾ˆå¤šæžç¬‘ç³»éƒ½é€™æ¨£
            "liquid_form",  # çœŸçš„æ˜¯æ°´/æ³¥æµåœ¨åœ°ä¸Šæµçš„
            # --- H. å·¨å¤§ / Boss / Enma / Shadowside ---
            "giant_boss",  # é«”ç© >> ç•«é¢ã€é ­è¶…å¤§ã€å¾€å¾€æ˜¯åŠ‡å ´ç‰ˆ/Blasters boss :contentReference[oaicite:17]{index=17}
            "winged_deity",  # Enma / ç¥žæ ¼åŒ– / æœ‰è£é£¾ç¿…çš„çŽ‹æ—åž‹
            "shadowside_monster",  # Shadowside ç‹€æ…‹ï¼šè‚¢é«”è®Šç²—ã€æ€ªç¸åŒ–/ç¸åŒ–
            "godside_extended",  # æœ‰ Godside å½¢æ…‹çš„é•·ç·šå¦–æ€ª â†’ ä¸»è¦æ˜¯å½¢æ…‹è†¨è„¹ç‰ˆ :contentReference[oaicite:18]{index=18}
            # --- I. æŠ½è±¡ / èƒ½é‡ / å½±å­ ---
            "shadow_silhouette",  # åªæœ‰è¼ªå»“ã€é»‘å½±ã€å½±åˆ†èº«
            "energy_aura",  # èƒ½é‡åœ˜ã€å…‰çƒã€ç«ç„°å½¢ â†’ å¾ˆå¤šçµ‚ç›¤å¦–æ€ª
            "symbolic_emblem",  # åªæœ‰ç¬¦è™Ÿã€åœ–é¨°ã€å¾½ç« åž‹
            "abstract",  # å¯¦åœ¨åˆ†ä¸å‡ºåž‹æ…‹æ™‚çš„ä¿åº•
        ]

    def detect_body_type(self, image: np.ndarray) -> str:
        """
        Automatically detect yokai body type

        Args:
            image: RGB image array

        Returns:
            Body type string
        """
        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        # Detect edges
        edges = cv2.Canny(gray, 50, 150)

        # Analyze image regions
        height, width = image.shape[:2]

        # Top region (head/wings)
        top_region = edges[: height // 3, :]
        top_density = top_region.sum() / (top_region.shape[0] * top_region.shape[1])

        # Middle region (body/limbs)
        mid_region = edges[height // 3 : 2 * height // 3, :]
        mid_density = mid_region.sum() / (mid_region.shape[0] * mid_region.shape[1])

        # Bottom region (legs/base)
        bottom_region = edges[2 * height // 3 :, :]
        bottom_density = bottom_region.sum() / (
            bottom_region.shape[0] * bottom_region.shape[1]
        )

        # Alpha channel analysis (if RGBA)
        if image.shape[2] == 4:
            alpha = image[:, :, 3]
            # Check for legs in bottom region
            bottom_alpha = alpha[2 * height // 3 :, :]
            has_bottom = (bottom_alpha > 128).sum() > (
                bottom_alpha.shape[0] * bottom_alpha.shape[1] * 0.2
            )
        else:
            has_bottom = True

        # Classify based on region densities
        if top_density > mid_density and top_density > bottom_density:
            if has_bottom:
                return "bipedal"  # Top-heavy with legs (bird-like)
            else:
                return "flying"  # Top-heavy without legs (flying)

        elif not has_bottom or bottom_density < 0.01:
            return "floating"  # No legs

        elif mid_density > top_density * 1.5:
            # Check horizontal spread
            mid_alpha = (
                image[height // 3 : 2 * height // 3, :, 3]
                if image.shape[2] == 4
                else gray[height // 3 : 2 * height // 3, :]
            )
            left_half = mid_alpha[:, : width // 2].sum()
            right_half = mid_alpha[:, width // 2 :].sum()
            if abs(left_half - right_half) < left_half * 0.3:
                return "quadruped"  # Symmetric horizontal spread

        # Default to humanoid
        return "humanoid"

    def extract_humanoid_pose(self, image: Image.Image) -> Optional[Dict]:
        """
        Extract humanoid pose using OpenPose

        Args:
            image: PIL Image

        Returns:
            Pose data dict or None
        """
        if not self.openpose:
            return None

        try:
            # Run OpenPose
            pose_img = self.openpose(image)
            pose_array = np.array(pose_img)

            # Calculate confidence (non-black pixel ratio)
            non_black = np.any(pose_array > 10, axis=2)
            confidence = non_black.sum() / (pose_array.shape[0] * pose_array.shape[1])

            return {
                "type": "humanoid",
                "pose_image": pose_array,
                "confidence": float(confidence),
                "has_body": confidence > 0.05,
                "has_limbs": confidence > 0.1,
            }

        except Exception as e:
            print(f"âš ï¸  OpenPose extraction failed: {e}")
            return None

    def extract_quadruped_pose(self, image: np.ndarray) -> Dict:
        """
        Extract quadruped (4-legged animal) pose

        Uses contour detection to find body and limb positions

        Args:
            image: RGB/RGBA image array

        Returns:
            Pose data dict
        """
        # Convert to grayscale
        if image.shape[2] == 4:
            # Use alpha channel
            gray = image[:, :, 3]
        else:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        # Threshold
        _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)

        # Find contours
        contours, _ = cv2.findContours(
            binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )

        if not contours:
            return {"type": "quadruped", "confidence": 0.0, "keypoints": []}

        # Get largest contour (main body)
        main_contour = max(contours, key=cv2.contourArea)

        # Find extreme points
        leftmost = tuple(main_contour[main_contour[:, :, 0].argmin()][0])
        rightmost = tuple(main_contour[main_contour[:, :, 0].argmax()][0])
        topmost = tuple(main_contour[main_contour[:, :, 1].argmin()][0])
        bottommost = tuple(main_contour[main_contour[:, :, 1].argmax()][0])

        # Estimate keypoints
        height, width = image.shape[:2]
        keypoints = {
            "head": topmost,
            "body_center": (int(width / 2), int(height / 2)),
            "front_left": (int(leftmost[0]), int(bottommost[1])),
            "front_right": (int(width * 0.4), int(bottommost[1])),
            "back_left": (int(width * 0.6), int(bottommost[1])),
            "back_right": (int(rightmost[0]), int(bottommost[1])),
        }

        # Create pose visualization
        pose_img = np.zeros((height, width, 3), dtype=np.uint8)

        # Draw skeleton
        # Spine
        cv2.line(
            pose_img, keypoints["head"], keypoints["body_center"], (255, 255, 255), 2
        )

        # Legs
        cv2.line(
            pose_img,
            keypoints["body_center"],
            keypoints["front_left"],
            (255, 255, 255),
            2,
        )
        cv2.line(
            pose_img,
            keypoints["body_center"],
            keypoints["front_right"],
            (255, 255, 255),
            2,
        )
        cv2.line(
            pose_img,
            keypoints["body_center"],
            keypoints["back_left"],
            (255, 255, 255),
            2,
        )
        cv2.line(
            pose_img,
            keypoints["body_center"],
            keypoints["back_right"],
            (255, 255, 255),
            2,
        )

        # Draw joints
        for point in keypoints.values():
            cv2.circle(pose_img, point, 3, (0, 255, 0), -1)

        confidence = cv2.contourArea(main_contour) / (height * width)

        return {
            "type": "quadruped",
            "pose_image": pose_img,
            "keypoints": keypoints,
            "confidence": float(confidence),
            "has_body": True,
            "has_limbs": True,
        }

    def extract_flying_pose(self, image: np.ndarray) -> Dict:
        """
        Extract flying creature pose (mainly orientation)

        Args:
            image: RGB/RGBA image array

        Returns:
            Pose data dict
        """
        # Get silhouette
        if image.shape[2] == 4:
            gray = image[:, :, 3]
        else:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)

        # Find contour
        contours, _ = cv2.findContours(
            binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )

        if not contours:
            return {"type": "flying", "confidence": 0.0}

        main_contour = max(contours, key=cv2.contourArea)

        # Fit ellipse to get orientation
        if len(main_contour) >= 5:
            ellipse = cv2.fitEllipse(main_contour)
            center = (int(ellipse[0][0]), int(ellipse[0][1]))
            angle = ellipse[2]
        else:
            M = cv2.moments(main_contour)
            if M["m00"] != 0:
                center = (int(M["m10"] / M["m00"]), int(M["m01"] / M["m00"]))
            else:
                center = (image.shape[1] // 2, image.shape[0] // 2)
            angle = 0

        # Create pose visualization (arrow showing direction)
        height, width = image.shape[:2]
        pose_img = np.zeros((height, width, 3), dtype=np.uint8)

        # Draw orientation arrow
        length = min(width, height) // 3
        rad = np.deg2rad(angle)
        end_x = int(center[0] + length * np.cos(rad))
        end_y = int(center[1] + length * np.sin(rad))

        cv2.arrowedLine(
            pose_img, center, (end_x, end_y), (255, 255, 255), 3, tipLength=0.3
        )
        cv2.circle(pose_img, center, 5, (0, 255, 0), -1)

        confidence = cv2.contourArea(main_contour) / (height * width)

        return {
            "type": "flying",
            "pose_image": pose_img,
            "center": center,
            "orientation": float(angle),
            "confidence": float(confidence),
        }

    def extract_pose(
        self, image_path: Path, body_type: Optional[str] = None
    ) -> Optional[Dict]:
        """
        Extract pose from image

        Args:
            image_path: Path to image
            body_type: Optional body type override

        Returns:
            Pose data dict or None
        """
        # Load image
        img = Image.open(image_path)

        # Convert to RGB/RGBA
        if img.mode not in ["RGB", "RGBA"]:
            img = img.convert("RGB")

        img_array = np.array(img)

        # Auto-detect body type if not specified
        if body_type is None:
            body_type = self.detect_body_type(img_array)

        # Extract pose based on body type
        if body_type == "humanoid":
            result = self.extract_humanoid_pose(img)
        elif body_type == "quadruped":
            result = self.extract_quadruped_pose(img_array)
        elif body_type in ["flying", "bipedal"]:
            result = self.extract_flying_pose(img_array)
        elif body_type == "floating":
            # Floating types: just center point
            height, width = img_array.shape[:2]
            pose_img = np.zeros((height, width, 3), dtype=np.uint8)
            center = (width // 2, height // 2)
            cv2.circle(pose_img, center, 10, (255, 255, 255), -1)
            result = {
                "type": "floating",
                "pose_image": pose_img,
                "center": center,
                "confidence": 0.8,
            }
        else:
            # Abstract or unknown
            return None

        if result:
            result["body_type"] = body_type
            result["image_path"] = str(image_path)

        return result

    def process_dataset(
        self,
        input_dir: Path,
        output_dir: Path,
        body_type: Optional[str] = None,
        min_confidence: float = 0.3,
    ) -> Dict:
        """
        Process entire dataset

        Args:
            input_dir: Input directory with images
            output_dir: Output directory
            body_type: Optional body type for all images
            min_confidence: Minimum pose confidence

        Returns:
            Processing statistics
        """
        print(f"\nðŸŽ­ Advanced Pose Extraction")
        print(f"  Input: {input_dir}")
        print(f"  Output: {output_dir}")
        print(f"  Body type: {body_type or 'auto-detect'}")
        print(f"  Min confidence: {min_confidence}")
        print()

        # Find images
        image_files = []
        for ext in ["*.png", "*.jpg", "*.jpeg"]:
            image_files.extend(input_dir.glob(ext))

        if not image_files:
            return {"success": False, "error": "No images found"}

        # Create output directories
        source_dir = output_dir / "source"
        pose_dir = output_dir / "pose"
        source_dir.mkdir(parents=True, exist_ok=True)
        pose_dir.mkdir(parents=True, exist_ok=True)

        stats = {
            "total_images": len(image_files),
            "processed": 0,
            "skipped_low_confidence": 0,
            "by_body_type": {},
        }

        results = []

        # Process each image
        for img_path in tqdm(image_files, desc="  Extracting poses"):
            try:
                # Extract pose
                pose_data = self.extract_pose(img_path, body_type)

                if not pose_data:
                    stats["skipped_low_confidence"] += 1
                    continue

                # Check confidence
                if pose_data.get("confidence", 0) < min_confidence:
                    stats["skipped_low_confidence"] += 1
                    continue

                # Save source image
                import shutil

                source_output = source_dir / img_path.name
                shutil.copy2(img_path, source_output)

                # Save pose image
                pose_output = pose_dir / img_path.name
                pose_img = pose_data.pop("pose_image")
                Image.fromarray(pose_img).save(pose_output)

                # Update statistics
                detected_type = pose_data.get("body_type", "unknown")
                if detected_type not in stats["by_body_type"]:
                    stats["by_body_type"][detected_type] = 0
                stats["by_body_type"][detected_type] += 1

                stats["processed"] += 1
                results.append(pose_data)

            except Exception as e:
                print(f"âš ï¸  Failed to process {img_path.name}: {e}")
                continue

        # Save metadata
        metadata = {
            "timestamp": datetime.now().isoformat(),
            "stats": stats,
            "results": results,
        }

        metadata_path = output_dir / "pose_metadata.json"
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2, default=str)

        print(f"\n{'='*80}")
        print("POSE EXTRACTION COMPLETE")
        print(f"{'='*80}")
        print(f"  Total images: {stats['total_images']}")
        print(f"  Processed: {stats['processed']}")
        print(f"  Skipped (low confidence): {stats['skipped_low_confidence']}")
        print()
        print("Body type distribution:")
        for body_type, count in sorted(
            stats["by_body_type"].items(), key=lambda x: x[1], reverse=True
        ):
            print(f"  {body_type}: {count}")
        print(f"{'='*80}\n")

        return {"success": True, "stats": stats}


def main():
    parser = argparse.ArgumentParser(
        description="Advanced pose extraction with yokai-specific handling"
    )

    parser.add_argument("input_dir", type=Path, help="Input directory with images")
    parser.add_argument(
        "--output-dir", type=Path, required=True, help="Output directory"
    )
    parser.add_argument(
        "--body-type",
        type=str,
        default=None,
        choices=[
            "humanoid",
            "quadruped",
            "bipedal",
            "flying",
            "floating",
            "multi_limbed",
        ],
        help="Body type (default: auto-detect)",
    )
    parser.add_argument(
        "--min-confidence",
        type=float,
        default=0.3,
        help="Minimum pose confidence (default: 0.3)",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=["cuda", "cpu"],
        help="Processing device (default: cuda)",
    )

    args = parser.parse_args()

    if not args.input_dir.exists():
        print(f"âŒ Input directory not found: {args.input_dir}")
        return

    # Initialize extractor
    extractor = AdvancedPoseExtractor(device=args.device)

    # Process dataset
    result = extractor.process_dataset(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        body_type=args.body_type,
        min_confidence=args.min_confidence,
    )

    if not result["success"]:
        print(f"âŒ {result.get('error', 'Processing failed')}")
        return

    print(f"Metadata saved: {args.output_dir / 'pose_metadata.json'}\n")


if __name__ == "__main__":
    main()
